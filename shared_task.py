# -*- coding: utf-8 -*-
"""shared task.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IlONNLhLvaw1faONETztJSlcCkSBqO_t
"""

from crf import load_data, make_labels2i

train_filepath = "./NER_TRAIN_JUDGEMENT.json"
dev_filepath = "./NER_DEV_JUDGEMENT.json"
train_sents, train_tag_sents = load_data(train_filepath)
dev_sents, dev_tag_sents = load_data(dev_filepath)
labels2i = make_labels2i()

print(len(train_sents))
print()
print("labels2i", labels2i)

from typing import List
def make_features(text: List[str], sent_tags) -> List[List[int]]:
    """Turn a text into a feature vector.

    Args:
        text (List[str]): List of tokens.

    Returns:
        List[List[int]]: List of feature Lists.
    """
    feature_lists = []
    for i, token in enumerate(text):
        feats = []
        # We add a feature for each unigram.
        if i > 0:
          prev_word = text[i-1]
          prev_pos  = sent_tags[i-1]
        else:
          prev_word = '<s>'
          prev_pos  = "<s>"
        if i < len(text)-1:
          next_word = text[i+1]
          next_pos  = sent_tags[i+1]
        else:
          next_word = '<s>'
          next_pos  = '<s>'
        feats.append(f"word={token}")
        feats.append(f"pos={sent_tags[i]}")
        feats.append(f"prev_word={prev_word}")
        feats.append(f"prev_pos={prev_pos}")
        feats.append(f"next_word={next_word}")
        feats.append(f"next_pos={next_pos}")
        
        # We append each feature to a List for the token.
        feature_lists.append(feats)
    return feature_lists

import spacy
def featurize(sents: List[List[str]]) -> List[List[List[str]]]:
    nlp = spacy.load("en_core_web_sm")
    feats = []
    for sent in sents:
        # Gets a List of Lists of feature strings
        # TO DO: Get pos tags
        sent_tags = []
        docs = [nlp(word) for word in sent]
        for doc in docs:
          for token in doc:
            sent_tags.append(token.pos_)
        feats.append(make_features(sent, sent_tags))
        #feats.append(make_features(sent, sent_tags))

    return feats

import torch
from crf import f1_score, predict, PAD_SYMBOL, pad_features, pad_labels
from tqdm.autonotebook import tqdm
import random

# TODO: Implement the training loop
# HINT: Build upon what we gave you for HW2.
# See cell below for how we call this training loop.

def training_loop(
    num_epochs,
    batch_size,
    train_features,
    train_labels,
    dev_features,
    dev_labels,
    optimizer,
    model,
    labels2i,
    pad_feature_idx
):
    # TODO: Zip the train features and labels
    samples = list(zip(train_features, train_labels))
    
    # TODO: Randomize them, while keeping them paired.
    random.shuffle(samples)
    
    # TODO: Build batches
    batches = []
    for i in range(0, len(samples), batch_size):
        batches.append(samples[i:i+batch_size])
    
    print("Training...")
    for i in range(num_epochs):
        losses = []
        for batch in tqdm(batches):
            # Here we get the features and labels, pad them,
            # and build a mask so that our model ignores PADs
            # We have abstracted the padding from you for simplicity, 
            # but please reach out if you'd like learn more.
            features, labels = zip(*batch)
            features = pad_features(features, pad_feature_idx)
            features = torch.stack(features)
            # Pad the label sequences to all be the same size, so we
            # can form a proper matrix.
            labels = pad_labels(labels, labels2i[PAD_SYMBOL])
            labels = torch.stack(labels)
            mask = (labels != labels2i[PAD_SYMBOL])
            
            # TODO: Empty the dynamic computation graph
            optimizer.zero_grad()


            # TODO: Run the model. Since we use the pytorch-crf model,
            # our forward function returns the positive log-likelihood already.
            # We want the negative log-likelihood. See crf.py forward method in NERTagger
            log_likelihood = model(features, labels, mask=mask)
            
            # TODO: Backpropogate the loss through our model
            negative_log_likelihood = -log_likelihood
            negative_log_likelihood.backward()

            # TODO: Update our coefficients in the direction of the gradient.
            optimizer.step()

            # TODO: Store the losses for logging
            losses.append(negative_log_likelihood.item())
        
        # TODO: Log the average Loss for the epoch
        print(f"epoch {i}, loss: {sum(losses)/len(losses)}")

        # TODO: make dev predictions with the `predict()` function
        # TODO: Compute F1 score on the dev set and log it.
        dev_f1 = f1_score(predict(model, dev_features), dev_labels, labels2i['O'])
        print(f"Dev F1 {dev_f1}")
        
    # Return the trained model
    return model

from crf import build_features_set
from crf import make_features_dict
from crf import encode_features, encode_labels
from crf import NERTagger

# Build the model and featurized data
print(len(train_sents[:500]))
train_features = featurize(train_sents[:500])
dev_features = featurize(dev_sents[:100])

# Get the full inventory of possible features
all_features = build_features_set(train_features)
# Hash all features to a unique int.
features_dict = make_features_dict(all_features)
# Initialize the model.
model = NERTagger(len(features_dict), len(labels2i))

encoded_train_features = encode_features(train_features, features_dict)
encoded_dev_features = encode_features(dev_features, features_dict)
encoded_train_labels = encode_labels(train_tag_sents, labels2i)
encoded_dev_labels = encode_labels(dev_tag_sents, labels2i)

# TODO: Play with hyperparameters here.
num_epochs = 100
batch_size = 16
LR=0.05
optimizer = torch.optim.SGD(model.parameters(), LR)

model = training_loop(
    num_epochs,
    batch_size,
    encoded_train_features,
    encoded_train_labels,
    encoded_dev_features,
    encoded_dev_labels,
    optimizer,
    model,
    labels2i,
    features_dict[PAD_SYMBOL]
)